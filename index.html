<!DOCTYPE html>
<html>

<head>
  <!-- <meta charset="utf-8">
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->
  <title>Automatic Teaching via Vision Language Retrieval Augmented Generation</title>
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <!-- <img id="painting_icon" width="5%" src="static/images/icon.png">  -->
              Automatic Teaching via Vision Language Retrieval
              Augmented Generation</h1>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                Ruslan Gokhman</a>,</span>
              <span class="author-block">
                Faaraan Farid Kazi</a>,</span>
              <span class="author-block">
                Jialu Li</a>,
              </span>
              <span class="author-block">
              Youshan Zhang</a>,
              </span>
              
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Yeshiva University, New York</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2403.13600.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/anonymous-user-2711/Info-flow-explanition" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <h2 class="title is-3">Abstract</h2>
          <!-- <img id="teaser" width="40%" src="static/examples/ills.png"> -->
          
          <div class="content has-text-justified">
            <p>
              Automating teaching presents unique challenges, as replicating human interaction and adaptability is complex. 
              Automated systems often lack the capacity to provide nuanced, real-time feedback that aligns with a studentâ€™s individual learning pace or comprehension level, which can hinder effective support for diverse needs. 
              This is especially challenging in fields where abstract concepts require adaptive explanations. In this paper, a vision language retrieval augmented generation (named VL-RAG) system has the potential to bridge this gap by delivering contextually relevant, visually enriched responses that enhance comprehension. 
              By leveraging a database of tailored answers and images, the VL-RAG system can dynamically retrieve information aligned with specific questions, creating a more interactive and engaging experience. It allows students to explore concepts visually and verbally, promoting deeper understanding and reducing the need for constant human oversight while maintaining flexibility to expand across different subjects and course material.
           </p>
            <p>

            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>




  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <!-- <h2 class="title is-3"><img id="painting_icon" width="3%" src="static/images/icon.png"> VL-Mamba</h2> -->
        <h2 class="title is-3"> Vision Language Retrieval Augmented Generation</h2>
      </div>
    </div>
    <!-- </div> https://cdn-icons-png.flaticon.com/512/5379/5379860.png -->
    <!--/ Results. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              In documenting the development and implementation of this RAG system web platform, the website will provide insights into the design considerations, technological frameworks, 
              and pedagogical strategies essential for integrating advanced VQA systems into higher education. Through a detailed analysis of the platform's impact on student engagement 
              and comprehension in initial deployments, we aim to offer a comprehensive evaluation of its efficacy and potential for broader application.
              This adaptive approach demonstrates the potential of the Automatic Teaching Platform Based on Vision Language RAG
              platform to become a cornerstone of educational technology within graduate programs, potentially transforming how complex scientific content is taught and learned
            </p>
          </div>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="static/images/MULTI-RAG-DIAGRAM.png">
              <br>
              <caption>Figure.1 Process Flow Architecture Diagram </caption>
            </div>


          </centering>
        </div>
      </div>



  </section>



  <section class="section">

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">

        <h2 class="title is-3"> Overview</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              The Vision-Language RAG model leverages advanced AI to integrate visual and textual data for a richer educational experience. 
              By blending Visual Question Answering (VQA) capabilities with powerful language models, it addresses complex topics more interactively, 
              providing students with an immersive learning tool.
            </p>
        
            <h3>Key Features</h3>
            <ul>
              <li><strong>Visual and Textual Integration</strong>: Combines visual inputs with text-based responses, offering a more holistic approach to understanding and engaging with content.</li>
              <li><strong>Interactive Question Answering</strong>: Allows students to interact with visual content directly, enhancing comprehension and retention by enabling them to ask and receive answers about images or visual data.</li>
              <li><strong>Advanced Language Models</strong>: Utilizes models like BART-Large CNN for accuracy and reliability, while also exploring LLAMA and T5 models for refining clarity and alignment with educational goals.</li>
            </ul>
        
            <h3>Benefits in Educational Settings</h3>
            <ul>
              <li><strong>Enhanced Student Engagement</strong>: Interactive features and visually driven answers create a more engaging and accessible way to learn complex subjects.</li>
              <li><strong>Improved Comprehension and Retention</strong>: By enabling students to interact with both visual and textual information, the model supports deeper learning and better retention of material.</li>
              <li><strong>Scalability Across Subjects</strong>: The modelâ€™s design supports potential expansion across multiple educational disciplines, from science and history to technical fields.</li>
            </ul>
        
            <h3>Current Challenges</h3>
            <ul>
              <li><strong>Model Selection for Clarity and Alignment</strong>: Models like LLAMA and T5 present challenges in achieving precise and aligned responses, which are critical in educational settings.</li>
              <li><strong>Infrastructure Demands</strong>: Implementing the model requires substantial computing power and accessibility features, which can limit adoption in some educational environments.</li>
            </ul>
        
            <h3>Future Directions</h3>
            <ul>
              <li><strong>Enhanced User Accessibility</strong>: Improving infrastructure and user accessibility to make the platform more available in varied educational contexts.</li>
              <li><strong>Expansion to Real-World Applications</strong>: Plans to develop hands-on, immersive experiences aligned with practical applications, bridging classroom learning and real-world relevance.</li>
              <li><strong>Adaptive Learning Capabilities</strong>: Future updates aim to make the model adaptive, providing students with tailored experiences that respond to individual learning needs and progress.</li>
            </ul>
        
            <p>
              This model represents a significant step forward in creating adaptive, interactive, and impactful learning environments 
              that can complement traditional education methods with AI-driven insights.
            </p>
          </div>
        </div>
      </div>



  </section>


  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">ðŸŽ¥ Video Demo</h2>
      </div>
    </div>
  
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-centered">
            <video width="100%" controls>
              <source src="static/videos/Automatic_Teaching.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
</section>

<!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section> -->
  
  
</body>

</html>
